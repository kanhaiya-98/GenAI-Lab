
# ðŸ¤– Local LLM Q\&A Chatbot

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?logo=python" />
  <img src="https://img.shields.io/badge/Streamlit-App-red?logo=streamlit" />
  <img src="https://img.shields.io/badge/LangChain-Orchestration-orange?logo=chainlink" />
  <img src="https://img.shields.io/badge/Ollama-LLM-black?logo=ollama" />
  <img src="https://img.shields.io/github/stars/your-username/your-repo-name?style=social" />
  <img src="https://img.shields.io/github/forks/your-username/your-repo-name?style=social" />
</p>

A **lightning-fast, fully local Q\&A chatbot** built with **Streamlit** and **Ollama**, allowing you to interact with open-source LLMs like **Llama 3**, **Phi-3**, and **Mistral** without internet or paid APIs.

ðŸ’¡ Perfect for research, coding help, or just testing LLMs locally with full privacy.

---

## âœ¨ Features

* ðŸ–¥ï¸ **Interactive Chat Interface** â€“ Smooth and modern UI for conversations
* ðŸ“œ **Session Chat History** â€“ Keep track of dialogue across sessions
* ðŸ  **Local LLM Support** â€“ Run Llama 3, Phi-3, Mistral entirely offline
* ðŸ”„ **Model Switching** â€“ Toggle between installed models via dropdown
* âš™ï¸ **Parameter Tuning** â€“ Control temperature & response length
* ðŸ”’ **Private & Secure** â€“ No data leaves your machine
* ðŸš€ **Modern Tools** â€“ Streamlit frontend + LangChain orchestration

---

## ðŸ›  Tech Stack

* **Framework:** Streamlit
* **LLM Orchestration:** LangChain
* **LLM Hosting:** Ollama
* **Language:** Python

---

## âš¡ Prerequisites

* Python 3.8+
* [Ollama](https://ollama.com/)
* An Ollama-compatible model (e.g., Llama 3, Phi-3, Mistral)

Pull Llama 3 after installing Ollama:

```bash
ollama pull llama3
```

---

## ðŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2ï¸âƒ£ Create a Virtual Environment

**Windows:**

```bash
python -m venv venv
venv\Scripts\activate
```

**macOS/Linux:**

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables (Optional)

Enable **LangSmith** for LLM tracking:

```bash
LANGCHAIN_API_KEY="your_langsmith_api_key_here"
LANGCHAIN_TRACING_V2="true"
```

> âš ï¸ Optional: Chatbot works perfectly without LangSmith.

---

## â–¶ï¸ Running the Application

```bash
streamlit run app.py
```

Access it locally at: [http://localhost:8501](http://localhost:8501)

ðŸ’¬ Chat, experiment, and explore **your local LLM** in action!

---

## âš™ï¸ How It Works

```mermaid
flowchart TD
    A["ðŸ“‚ Upload PDFs / Input Text"] --> B["ðŸ”Ž Chunking & Embedding"]
    B --> C["ðŸ—„ï¸ Store in Vector DB (ChromaDB)"]
    C --> D["ðŸ“œ Query Reformulation / Context-Aware"]
    D --> E["ðŸ” Similarity Search in Vector DB"]
    E --> F["âš¡ Groq LLM / Ollama LLM Processing"]
    F --> G["ðŸ¤– Generate Answer / Response"]
    G --> H["ðŸ’¬ Display Answer in Chat UI"]

    classDef step fill:#f3f4f6,stroke:#333,stroke-width:1px,rx:10,ry:10,font-size:12px;
    class A,B,C,D,E,F,G,H step;
```

> This workflow shows **input â†’ embedding â†’ retrieval â†’ LLM â†’ output**, giving users an intuitive overview at a glance.

---

## ðŸŒˆ Screenshots

<p align="center">
  <img src="assets/screenshot1.png" width="45%" />
  <img src="assets/screenshot2.png" width="45%" />
</p>

---

## ðŸ’Ž Pro Tips

* Experiment with **different models** to see varied creativity
* Use **temperature** and **max tokens** to fine-tune responses
* Keep your **models updated locally** for best performance

---



